{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Individual Notebook Health Insurance Cross Sell Prediction",
      "provenance": [],
      "collapsed_sections": [
        "GcPhaic1PmRC",
        "VVOSiAhHPmRO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avisikta-Majumdar/Capstone-Project_Health_Insurance_Cross_Sell_Prediction/blob/main/Individual_Notebook_Health_Insurance_Cross_Sell_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGcl5Bv9ed6u"
      },
      "source": [
        "# **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJAmGx75jJk1"
      },
      "source": [
        "Our client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.\n",
        "\n",
        "An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.\n",
        "\n",
        "For example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.\n",
        "\n",
        "Just like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called ‘sum assured’) to the customer.\n",
        "\n",
        "Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n",
        "\n",
        "Now, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzGDqdC4fZ-b"
      },
      "source": [
        "# **Attribute Information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJxl68MjYbs"
      },
      "source": [
        "1. id :\tUnique ID for the customer\n",
        "\n",
        "2. Gender\t: Gender of the customer\n",
        "\n",
        "3. Age :\tAge of the customer\n",
        "\n",
        "4. Driving_License\t0 : Customer does not have DL, 1 : Customer already has DL\n",
        "\n",
        "5. Region_Code :\tUnique code for the region of the customer\n",
        "\n",
        "6. Previously_Insured\t: 1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance\n",
        "\n",
        "7. Vehicle_Age :\tAge of the Vehicle\n",
        "\n",
        "8. Vehicle_Damage\t :1 : Customer got his/her vehicle damaged in the past. 0 : Customer didn't get his/her vehicle damaged in the past.\n",
        "\n",
        "9. Annual_Premium\t: The amount customer needs to pay as premium in the year\n",
        "\n",
        "10. PolicySalesChannel :\tAnonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n",
        "\n",
        "11. Vintage :\tNumber of Days, Customer has been associated with the company\n",
        "\n",
        "12. Response :\t1 : Customer is interested, 0 : Customer is not interested"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW-bnj77PmQ5"
      },
      "source": [
        "# <font size=\"+2\" color='#ff3ba8'><b><i><u>Contents</u>\n",
        "* <font size=\"+2\" color='#053c96'> <b>Importing Libraries\n",
        "* <font size=\"+2\" color='#053c96'> <b>Import Data\n",
        "* <font size=\"+2\" color='#053c96'> <b>Data Summary\n",
        "* <font size=\"+2\" color='#053c96'> <b>Data Visualization\n",
        "* <font size=\"+2\" color='#053c96'> <b>Data Cleaning ( EDA )\n",
        "* <font size=\"+2\" color='#053c96'> <b>Feature Selection\n",
        "* <font size=\"+2\" color='#053c96'> <b> Model Selection\n",
        "* <font size=\"+2\" color='#053c96'><b> Hyperparameter Tuning\n",
        "* <font size=\"+2\" color='#053c96'><b>Conclusion\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7YFgUPtPmQ6",
        "outputId": "1e4fbba9-1d1d-40c1-e7d7-0ec2242ba57e"
      },
      "source": [
        "!pip install imblearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (from imblearn) (0.8.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rOhFsszPmQ7",
        "outputId": "544bf0c3-9031-4f55-fca3-ed6724b434aa"
      },
      "source": [
        "!pip install xgboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxrb2u3DPmQ8"
      },
      "source": [
        "## 1.Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtFqjkKAPmQ9"
      },
      "source": [
        "# import libraries\n",
        "import pandas  as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Used in data preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#used to split dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#used to resampling(when our dependent variable is imbalanced)\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "#Ml algorithms\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "#used in feature selection\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_score \n",
        "from sklearn.metrics import recall_score \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "sns.set_theme(style=\"darkgrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCLF1wTKPmQ-"
      },
      "source": [
        "## 2. Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nma7LzYmP2FY"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpjvYNUEPmQ-"
      },
      "source": [
        "path = \"/content/drive/MyDrive/AlmaBetter/Team Capstone Projects/Submitted Projects/3. Classification ( Health Insurance Cross Sell Prediction )/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv\"\n",
        "df = pd.read_csv(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKO7Oy9wPmQ-"
      },
      "source": [
        "## 3. Data Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### df.head()"
      ],
      "metadata": {
        "id": "d_0hyFYXzfqy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG2QhzcNvsAo"
      },
      "source": [
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### df.tail()"
      ],
      "metadata": {
        "id": "jV30YClNznPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "LBQXFIchznDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### df.info()"
      ],
      "metadata": {
        "id": "rcGXq_IBzqFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info(memory_usage = 'deep')"
      ],
      "metadata": {
        "id": "Dwf3zegYTCOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### df.shape"
      ],
      "metadata": {
        "id": "ourYV3dvzs-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "t0Ewee9yq4qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### df.columns"
      ],
      "metadata": {
        "id": "2ScGlLBVzvJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "noxaF-qnq5qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJcmv6HTfnnz"
      },
      "source": [
        "###  Dataset details\n",
        "*   A new *DataFrame* where we have columns name of this df along with datatype , missing value no ,  unique values no , first value , second value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1lv8cXkPmQ_"
      },
      "source": [
        "def DataInfoAll(df):\n",
        "    print(f\"Dataset Shape: {df.shape}\")\n",
        "    print(\"-\"*75)\n",
        "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
        "    summary = summary.reset_index()\n",
        "    summary['Name'] = summary['index']\n",
        "    summary = summary[['Name','dtypes']]\n",
        "    summary['Missing'] = df.isnull().sum().values    \n",
        "    summary['Uniques'] = df.nunique().values\n",
        "    summary['First Value'] = df.iloc[0].values\n",
        "    summary['Second Value'] = df.iloc[1].values\n",
        "    return summary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQcDELJWPmQ_"
      },
      "source": [
        "DataInfoAll(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_QqajjqPmQ_"
      },
      "source": [
        "There is no Null value present in this dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ImYfV4uPmRA"
      },
      "source": [
        "All the numerical values are present in integer or float datatype , <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lafCYnCfffgK"
      },
      "source": [
        "### Checking outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tXF7FJYemRs"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df1 = df[list(df.describe())]\n",
        "\n",
        "for column in df1:\n",
        "        plt.figure(figsize=( 8 , 8))\n",
        "        plt.title(f'Boxplot for {column}' , fontsize = 15)\n",
        "        sns.boxplot(data=df1, x=column)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking duplicate values "
      ],
      "metadata": {
        "id": "SjtA_18o0R6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate = df[df.duplicated()]\n",
        "print(f\"There are {duplicate.shape[0]} duplicate rows present in the dataset.\")"
      ],
      "metadata": {
        "id": "pwJ4mx370VwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking NaN values"
      ],
      "metadata": {
        "id": "5hNaNFvC0WEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum().to_frame().T"
      ],
      "metadata": {
        "id": "-2BIm5RR0WXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HeSbGflPmRA"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZg1h_bnPmRA"
      },
      "source": [
        "### Target Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99KMSnlyPmRB"
      },
      "source": [
        "sns.set_theme(style=\"darkgrid\")\n",
        "sns.countplot(df['Response'] , data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcPhaic1PmRC"
      },
      "source": [
        "### The data is highly imbalanced.\n",
        "As you can see in above graph, there are very few interested customers whose stats are less than 50000 and those above 300000 are not interested"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uTnj6f3iCDb"
      },
      "source": [
        "### Let's check outlier present in all numerical columns "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERnRkCrpiPEL"
      },
      "source": [
        "plt.rcParams['figure.figsize']=(20,12)\n",
        "ax = df[list(df.describe())].plot(kind='box', title='Boxplot', showmeans=True)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v42Sor1CiaVa"
      },
      "source": [
        "#### As you can see\n",
        "* ##### **Annual_Premium** has the highest outliers present in this dataset\n",
        "* ##### **Driving_License** has very less outliers.\n",
        "* ##### **Response** has very less outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL9aWJ5GPmRC"
      },
      "source": [
        "### Gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew3dWUNePmRC"
      },
      "source": [
        "plt.figure(figsize = (13,5))\n",
        "plt.subplot(1,2,1)\n",
        "sns.countplot(df['Gender'],palette='husl')\n",
        "plt.title(\"Count of Male & Female\")\n",
        "plt.subplot(1,2,2)\n",
        "sns.countplot(df['Gender'], hue = df['Response'],palette=\"husl\")\n",
        "plt.title(\"Response in Male and Female Category\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWAKtu8BPmRC"
      },
      "source": [
        "* The gender variable ratio in the dataset is almost equal, male category is slightly more than female and also the chances of buying insurance is also little high than female.<br><br>\n",
        "* The number of male is greater than 200000 and The number of female is close to 175000. The number of male is intersted which is greater than 25000 and The number of female is intersted which is below 25000.Male category is slightly greater than that of female and chances of buying the insurance is also little high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwVjHCX7PmRC"
      },
      "source": [
        "### Age vs Response"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCdgfmCkPmRD"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWyCZnNsPmRD"
      },
      "source": [
        "#### Age VS Response\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.countplot(x='Age',hue='Response',data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXMZuIxXPmRD"
      },
      "source": [
        "### Checking is there outlier present or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv4-PHSwPmRD"
      },
      "source": [
        "sns.boxplot(df['Age'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhBIY0U1PmRE"
      },
      "source": [
        "* Young people below 30 are not interested in vehicle insurance. Reasons could be lack of experience, less maturity level and they don't have expensive vehicles yet.\n",
        "* People aged between 30-60 are more likely to be interested.\n",
        "* From the boxplot we can see that there no outlier in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoqX505tPmRE"
      },
      "source": [
        "As you can see there is no outliers present in **Age**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQRcKC0wPmRE"
      },
      "source": [
        "df.Driving_License.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvwbW2_CPmRE"
      },
      "source": [
        "plt.figure( figsize = (10 , 6))\n",
        "sns.countplot(df['Driving_License'],hue=df['Response'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9Gh4AjYPmRF"
      },
      "source": [
        "* Customers who are interested in Vehicle Insurance almost all have driving license"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6kgu2BMPmRF"
      },
      "source": [
        "### Previously_Insured Vs Response"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecL1CFgcPmRF"
      },
      "source": [
        "plt.figure( figsize = (10 , 6))\n",
        "sns.countplot(x = 'Previously_Insured' , hue = 'Response' , data = df , palette = 'husl' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItAaiShkPmRF"
      },
      "source": [
        "* Those who have not insurance some of them are taking insurance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtJktGcOPmRG"
      },
      "source": [
        "### Vehicle_Age Vs Response"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GvuDSdnPmRG"
      },
      "source": [
        "df.Vehicle_Age.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMrALKLOPmRG"
      },
      "source": [
        "plt.figure( figsize = (10 , 6))\n",
        "sns.countplot(x = 'Vehicle_Age' , hue = 'Response' , data = df , palette = 'husl')\n",
        "plt.axis([None,None,10,175000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKygW4ZRPmRG"
      },
      "source": [
        "* From seeing this graph we can say that if the vehicle's age is in between 1 to 2 years ,those vehicle owners are more likely to buy insurance<br><br>\n",
        "* No of customers with Vehicle_Age >2 is more than the no of customers whose Vehicle_Age< 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6Kt0AcjPmRG"
      },
      "source": [
        "### Annual_Premium"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvAMVpF4PmRG"
      },
      "source": [
        "plt.figure(figsize=(13,7))\n",
        "plt.subplot(2,1,1)\n",
        "sns.distplot(df['Annual_Premium'], color='green')\n",
        "plt.title(\"Distribution of Annual premium\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb2EFZaqPmRH"
      },
      "source": [
        "* From the distribution plot we can infer that **the annual premimum variable is right skewed.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT6tNeNFPmRH"
      },
      "source": [
        "plt.figure(figsize=(13,7))\n",
        "sns.boxplot(df['Annual_Premium'])\n",
        "plt.title(\"boxplot of Annual premium\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ECUgXPPmRH"
      },
      "source": [
        "* As you can see that in the column **Annual_premium** there are many outliers present"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4B95BLFPmRH"
      },
      "source": [
        "### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BDxyLr7LPmRH"
      },
      "source": [
        "corr = df.corr()\n",
        "\n",
        "f, ax = plt.subplots(figsize = (8 , 8 ))\n",
        "\n",
        "sns.heatmap(corr, ax=ax, annot=True,linewidths=3,cmap='YlGn')\n",
        "\n",
        "plt.title(\"Pearson correlation of Features\", fontsize=25 ,y=1.05, size=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvEfFrXLPmRH"
      },
      "source": [
        "* **Target variable ( Response )** is not much affected by Vintage variable. we can drop least correlated variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKKsnmIhPmRH"
      },
      "source": [
        "## Data Cleaning ( EDA )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze8oJrFZPmRI"
      },
      "source": [
        "#### Removing duplicate rows "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY4lIiipPmRI"
      },
      "source": [
        "df_old_row = df.shape[0]\n",
        "df.drop_duplicates(inplace = True)\n",
        "df_new_row = df.shape[0]\n",
        "if df_old_row == df_new_row:\n",
        "    print(\"There was no duplicate rows present\")\n",
        "else:\n",
        "    print(f\"There was {df_old_row - df_new_row} duplicate rows present\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8v1JUxjPmRI"
      },
      "source": [
        "numerical_cols = list(df.describe())\n",
        "numerical_df = df[numerical_cols]\n",
        "numerical_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj5tCSU6PmRI"
      },
      "source": [
        "categorical_cols = list(set(df.columns) - set(numerical_cols))\n",
        "categorical_df = df[categorical_cols]\n",
        "categorical_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p74gR2snPmRI"
      },
      "source": [
        "Let's convert the categorical columns into numeric using **LabelEncoder**,<br>But before that let's check in each column of categorical_df how namy unique values are present\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VWEiooLPmRI"
      },
      "source": [
        "for column_name in categorical_df.columns:\n",
        "    print('-'*35)\n",
        "    print(categorical_df[column_name].value_counts(),'\\n')\n",
        "    print('-'*35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o57YOFqjowKO"
      },
      "source": [
        "#### Using LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-pGDdCSoxE5"
      },
      "source": [
        "categorical_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjD0US0cPmRJ"
      },
      "source": [
        "le = LabelEncoder()\n",
        "categorical_df = categorical_df.apply(le.fit_transform)\n",
        "categorical_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiLIwOM7pV3_"
      },
      "source": [
        "## Let's check the classes of label encoder\n",
        "le.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPqJv_0QpfiP"
      },
      "source": [
        "##Let's use inverse_transform\n",
        "le.inverse_transform([1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dDHauhCPmRJ"
      },
      "source": [
        "categorical_df_new = categorical_df\n",
        "categorical_df_new.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEIVZ_WkPmRJ"
      },
      "source": [
        "Let's make new **df** by merging *numerical_df* DataFrame with *categorical_df_new*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtTlxroAPmRJ"
      },
      "source": [
        "df = pd.merge( numerical_df , categorical_df_new , left_index = True , right_index = True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJTCCu8MPmRJ"
      },
      "source": [
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5su6RggsPmRK"
      },
      "source": [
        "**id** column is having the insurance id number, it will not help us to prediction, that's why I'm dropping this column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R55m--FsPmRK"
      },
      "source": [
        "df = df.drop( axis=1 , columns = ['id'])\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dis1SJPIPmRK"
      },
      "source": [
        "DataInfoAll(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ8WjxDTPmRK"
      },
      "source": [
        "### **Seprating dependent and independent variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeTWpnJwPmRK"
      },
      "source": [
        "x = df.drop(columns = ['Response'])\n",
        "y = df.Response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgkhmiEvPmRL"
      },
      "source": [
        "x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCUwostNPmRL"
      },
      "source": [
        "y.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqGbPo7JPmRL"
      },
      "source": [
        "##  **Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMeGNRuGPmRL"
      },
      "source": [
        "# Building the model\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,criterion ='entropy', max_features = 2)\n",
        "\n",
        "# Training the model\n",
        "extra_tree_forest.fit(x, y)\n",
        "\n",
        "# Computing the importance of each feature\n",
        "feature_importance = extra_tree_forest.feature_importances_\n",
        "\n",
        "# Normalizing the individual importances\n",
        "feature_importance_normalized = np.std( [ tree.feature_importances_ for tree in extra_tree_forest.estimators_ ] , axis = 0)\n",
        "\n",
        "\n",
        "# Plotting a Bar Graph to compare the models\n",
        "plt.figure(figsize = (24,12))\n",
        "plt.bar(x.columns, feature_importance_normalized)\n",
        "plt.xlabel('Feature Labels' , fontsize = 25)\n",
        "plt.ylabel('Feature Importances' , fontsize = 25)\n",
        "plt.title('Comparison of different Feature Importances' , fontsize = 45)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUap4WQ3PmRL"
      },
      "source": [
        "feat_importances_Series = pd.Series( feature_importance_normalized , index=x.columns)\n",
        "print(\"Feature Name\\t\\t Importance\")\n",
        "print(\"-\"*37 , end='\\n')\n",
        "feat_importances_Series.sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhEnPKpBPmRM"
      },
      "source": [
        "* We can **remove less important features from the data set**\n",
        "* *Driving_License , Gender* is contributing very less that's why I'm removing those columns "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn37TFKiPmRM"
      },
      "source": [
        "x.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyIzuwhhPmRM"
      },
      "source": [
        "x.drop( labels = [ 'Driving_License'  , 'Gender' ] , axis = 1 , inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m97L-0hNPmRN"
      },
      "source": [
        "x.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjgXTH2mPmRN"
      },
      "source": [
        "### Handling Imbalanced data\n",
        "* *When observation in one class is higher than the observation in other classes then there exists a class imbalance. We can clearly see that there is a huge difference between the data set. Solving this issue we use resampling technique*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltg9ECCfPmRN"
      },
      "source": [
        "type(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zsuCqIzDo0y"
      },
      "source": [
        "### Using **RandomOverSampler** to resample the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJLQ_jx1HMOA"
      },
      "source": [
        "\"HEALTH INSURANCE CROSS SELL PREDICTION\".title()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tQa2jtIPmRO"
      },
      "source": [
        "randomsample=  RandomOverSampler(random_state = 1)\n",
        "x_new,y_new=randomsample.fit_resample(x,y)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize = (13,5))\n",
        "plt.subplot(1,2,1)\n",
        "sns.countplot(y,palette='husl')\n",
        "\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "print('Original dataset shape {}'.format(Counter(y)))\n",
        "print('Resampled dataset shape {}'.format(Counter(y_new)))\n",
        "plt.subplot(1,2,2)\n",
        "sns.countplot(y_new,palette='husl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY7Kb8msPmRO"
      },
      "source": [
        "* As you can see now our response is having same no of both classes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVOSiAhHPmRO"
      },
      "source": [
        "### Splitting Dataset into 80:20 ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dahmAb4XPmRO"
      },
      "source": [
        "#dividing the dataset into training and testing\n",
        "xtrain , xtest , ytrain , ytest = train_test_split( x_new , y_new , test_size = 0.2 , random_state = 1 )\n",
        "print(f\"xtrain.shape\\txtest.shape\\tytrain.shape\\tytest.shape\")\n",
        "print('-'*60)\n",
        "print(f'{xtrain.shape}\\t{xtest.shape}\\t {ytrain.shape}\\t {ytest.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZPUWoIWPmRO"
      },
      "source": [
        "## **Feature Scaling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41SEou7pPmRO"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "xtrain = scaler.fit_transform(xtrain)\n",
        "xtest = scaler.transform(xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAcGI6yKPmRP"
      },
      "source": [
        "## **Model Selection**\n",
        "#### Problem can be identified as Binary Classification (wheather customer opts for vehicle insurance or not)\n",
        "#### Dataset has more than 300k records\n",
        "#### cannot go with SVM Classifier as it takes more time to train as dataset increase\n",
        "\n",
        "#### Idea is to start selection of models as:\n",
        "\n",
        "### **1. Logistic Regression**\n",
        "### **2. Random Forest**\n",
        "### **3. XGBClassifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRVwY7J_PmRP"
      },
      "source": [
        "### <font size = +2 color = #2718d3> 1.Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw5v0UlcPmRP"
      },
      "source": [
        "model=LogisticRegression()\n",
        "\n",
        "model=model.fit(xtrain,ytrain)\n",
        "\n",
        "pred=model.predict(xtest)\n",
        "\n",
        "lr_probability =model.predict_proba(xtest)[:,1]\n",
        "\n",
        "\n",
        "acc_lr=accuracy_score(ytest,pred)\n",
        "recall_lr=recall_score(ytest,pred)\n",
        "precision_lr=precision_score(ytest,pred)\n",
        "f1score_lr=f1_score(ytest,pred)\n",
        "AUC_LR=roc_auc_score(pred,ytest)\n",
        "\n",
        "#print accuracy and Auc values of model\n",
        "print(\"Accuracy : \", round(accuracy_score(ytest,pred) , 3))\n",
        "print(\"Precision:\" , round(precision_score(ytest,pred) , 3))\n",
        "print(\"Recall:\" , round(recall_score(ytest,pred), 3))\n",
        "print(\"F1-Score:\" , round(f1_score(ytest,pred) , 3))\n",
        "print(\"ROC_AUC Score:\" , round(AUC_LR , 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqS7goNqPmRP"
      },
      "source": [
        "print(classification_report(pred,ytest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdA-z2KkPmRQ"
      },
      "source": [
        "#### ROC curve for logistic reg."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkcNvwOKPmRQ"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, _ = roc_curve(ytest, lr_probability)\n",
        "plt.figure( figsize = (4 ,4))\n",
        "plt.title('Logistic Regression ROC curve')\n",
        "plt.xlabel('FPR (Precision)')\n",
        "plt.ylabel('TPR (Recall)')\n",
        "\n",
        "\n",
        "plt.plot(fpr,tpr)\n",
        "plt.plot((0,1), ls='dashed',color='black')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbTGcLNzPmRQ"
      },
      "source": [
        "#### Confusion Matrix for Logistic Reg."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL6wCitpPmRQ"
      },
      "source": [
        "cm=confusion_matrix(ytest,pred)\n",
        "print(cm)\n",
        "plt.figure( figsize = (4 ,4))\n",
        "sns.heatmap(cm,annot=True,cmap='BuPu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgiudiiNPmRR"
      },
      "source": [
        "### <font size = +2 color = #2718d3> 2.RandomForest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOqNbN2mPmRR"
      },
      "source": [
        "randomforest = RandomForestClassifier()\n",
        "\n",
        "randomforest=randomforest.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred = randomforest.predict(xtest)\n",
        "\n",
        "RF_probability = randomforest.predict_proba(xtest)[:,1]\n",
        "\n",
        "\n",
        "\n",
        "AUC_RF=roc_auc_score(y_pred,ytest)\n",
        "acc_rf=accuracy_score(ytest,y_pred)\n",
        "recall_rf=recall_score(ytest,y_pred)\n",
        "precision_rf=precision_score(ytest,y_pred)\n",
        "f1score_rf=f1_score(ytest,y_pred)\n",
        "\n",
        "#print accuracy and Auc values of model\n",
        "print(\"Accuracy : \", round(accuracy_score(ytest , y_pred) , 3))\n",
        "print(\"Precision:\" , round(precision_score(ytest,y_pred) , 3))\n",
        "print(\"Recall:\" , round(recall_score(ytest , y_pred), 3))\n",
        "print(\"F1-Score:\" , round(f1_score(ytest , y_pred) , 3))\n",
        "print(\"ROC_AUC Score:\" , round(AUC_LR , 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufpISyYoPmRR"
      },
      "source": [
        "print(classification_report(y_pred,ytest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZbABRNBrImo"
      },
      "source": [
        "#### ROC curve for RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt1YJ1QYPmRR"
      },
      "source": [
        "fpr, tpr, _ = roc_curve(ytest, RF_probability)\n",
        "plt.figure( figsize = (4 , 4))\n",
        "plt.title('RF Classifier ROC curve')\n",
        "plt.xlabel('FPR (Precision)')\n",
        "plt.ylabel('TPR (Recall)')\n",
        "\n",
        "plt.plot(fpr,tpr)\n",
        "plt.plot((0,1), ls='dashed',color='black')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Ty8RfRPmRS"
      },
      "source": [
        "#### Confusion Matrix for Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bNf6xZpPmRS"
      },
      "source": [
        "cm=confusion_matrix(y_pred,ytest)\n",
        "print(cm)\n",
        "plt.figure( figsize = (4 , 4))\n",
        "sns.heatmap(cm,annot=True,cmap='RdPu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm-iKKipPmRS"
      },
      "source": [
        "### <font size = +2 color = #2718d3>3. XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWgDhj7yPmRS"
      },
      "source": [
        "xgb=XGBClassifier()\n",
        "\n",
        "XGB_fit=xgb.fit(xtrain, ytrain)\n",
        "\n",
        "y_predict = XGB_fit.predict(xtest)\n",
        "\n",
        "XGB_probability = XGB_fit.predict_proba(xtest)[:,1]\n",
        "\n",
        "\n",
        "\n",
        "acc_xgb = accuracy_score( ytest , y_predict)\n",
        "recall_xgb = recall_score( ytest , y_predict)\n",
        "precision_xgb = precision_score( ytest , y_predict)\n",
        "f1score_xgb = f1_score( ytest , y_predict)\n",
        "\n",
        "AUC_xgb = roc_auc_score( y_predict , ytest)\n",
        "\n",
        "\n",
        "#print accuracy and Auc values of model\n",
        "print(\"Accuracy : \", round(acc_xgb , 3))\n",
        "print(\"Precision:\" , round(precision_xgb , 3))\n",
        "print(\"Recall:\" , round( recall_xgb , 3))\n",
        "print(\"F1-Score:\" , round( f1score_xgb , 3))\n",
        "print(\"ROC_AUC Score:\" , round(AUC_xgb , 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47wzU_ZWPmRT"
      },
      "source": [
        "print(classification_report( y_predict , ytest ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYrRxGB8rTNn"
      },
      "source": [
        "#### ROC curve for XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi7AziJJPmRT"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, _ = roc_curve(ytest, XGB_probability)\n",
        "plt.figure( figsize = (4 ,4))\n",
        "\n",
        "plt.title('XGBoost ROC curve')\n",
        "plt.xlabel('FPR (Precision)')\n",
        "plt.ylabel('TPR (Recall)')\n",
        "\n",
        "plt.plot(fpr,tpr)\n",
        "plt.plot((0,1), ls='dashed',color='black')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYyZsHRTaAc-"
      },
      "source": [
        "#### Confusion Matrix for XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtnpJFgYPmRT"
      },
      "source": [
        "#it helps to identify how many values are classified correctly\n",
        "cm=confusion_matrix(ytest,y_predict)\n",
        "print(cm)\n",
        "plt.figure( figsize = ( 4 , 4 ))\n",
        "sns.heatmap(cm,annot=True,cmap='GnBu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE3LfEvNPmRT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y8_W9SiPmRT"
      },
      "source": [
        "##  <font size = '+2'  color = '#c4d318'><b> Let's compare the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiCcH2WXPmRT"
      },
      "source": [
        "ind=['Logistic regression','RandomForest','XGBClassifier']\n",
        "data={\"Accuracy\":[acc_lr,acc_rf,acc_xgb],\"Recall\":[recall_lr,recall_rf,recall_xgb],\"Precision\":[precision_lr,precision_rf,precision_xgb],\n",
        "    'f1_score':[f1score_lr,f1score_rf,f1score_xgb],\"ROC_AUC\":[AUC_LR,AUC_RF,AUC_xgb]}\n",
        "result=pd.DataFrame(data=data,index=ind)\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjPBZTiMPmRU"
      },
      "source": [
        "## <font size='+2' color = '#2441ff'> Hyperparameter Tuning\n",
        "* <i>RandomForestClassifier is giving highest accuracy , that's why by using GridSearchCV I will set Hyperparameters value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkplw0WHPmRU"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score , ShuffleSplit , GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WBvV3jPPmRV"
      },
      "source": [
        "Model = RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaoDERCqPmRV"
      },
      "source": [
        "params = {\n",
        "    'n_estimators': [5,10,25],\n",
        "    'criterion':[\"gini\", \"entropy\"],\n",
        "    'max_depth' : [5,25,50],\n",
        "    'min_samples_split':[2,15,45]\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbifGpgOPmRV"
      },
      "source": [
        "gridsearch = GridSearchCV(Model , params , cv=5, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzTrcL--PmRV"
      },
      "source": [
        "gridsearch.fit(xtrain , ytrain )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgoiFxHXPmRV"
      },
      "source": [
        "print(gridsearch.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X41jXu55PmRW"
      },
      "source": [
        "Now we got the best values of our hyperparameters,<br>\n",
        "* #### Let's *build* the **FinalModel**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "878JP8HIty4e"
      },
      "source": [
        "gridsearch_predictions = gridsearch.predict( xtest ) \n",
        "  \n",
        "# print classification report \n",
        "print(classification_report(ytest, gridsearch_predictions)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whR7_YQPPmRW"
      },
      "source": [
        "Grid_predict_proba = gridsearch.predict_proba(xtest)[:,1]\n",
        "\n",
        "AUC_RF_Best = roc_auc_score(gridsearch_predictions ,ytest)\n",
        "acc_rf_Best = accuracy_score(gridsearch_predictions ,y_pred)\n",
        "recall_rf_Best = recall_score(gridsearch_predictions ,y_pred)\n",
        "precision_rf_Best = precision_score(ytest,gridsearch_predictions )\n",
        "f1score_rf_Best = f1_score(ytest,gridsearch_predictions )\n",
        "\n",
        "#print accuracy and Auc values of model\n",
        "print(\"Accuracy : \", round(accuracy_score(ytest,gridsearch_predictions) , 3))\n",
        "print(\"Precision:\" , round(precision_score(ytest,gridsearch_predictions) , 3))\n",
        "print(\"Recall:\" , round(recall_score(ytest, gridsearch_predictions ), 3))\n",
        "print(\"F1-Score:\" , round(f1_score(ytest, gridsearch_predictions ) , 3))\n",
        "print(\"ROC_AUC Score:\" , round(AUC_LR , 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYIHpVGPPmRW"
      },
      "source": [
        "print(classification_report(y_pred , gridsearch_predictions ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcrzZkaDPmRW"
      },
      "source": [
        "fpr, tpr, _ = roc_curve(ytest, Grid_predict_proba)\n",
        "\n",
        "plt.figure( figsize = ( 6 , 6 ))\n",
        "plt.title('RF Classification(with Hyperparameters) ROC curve')\n",
        "plt.xlabel('FPR (Precision)')\n",
        "plt.ylabel('TPR (Recall)')\n",
        "\n",
        "plt.plot(fpr,tpr)\n",
        "plt.plot((0,1), ls='dashed',color='black')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWEibg7gPmRX"
      },
      "source": [
        "cm=confusion_matrix(y_pred,ytest)\n",
        "print(cm)\n",
        "plt.figure( figsize = ( 4 , 4 ))\n",
        "sns.heatmap(cm,annot=True,cmap='RdPu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lPBKqFKPmRX"
      },
      "source": [
        "## <b>Final Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClVOdVe7PmRX"
      },
      "source": [
        "ind=['RandomForest','RandomForest(Using Hyper.)']\n",
        "data={\"Accuracy\":[acc_rf , acc_rf_Best],\"Recall\":[ recall_rf , recall_rf_Best],\"Precision\":[ precision_rf , precision_rf_Best],\n",
        "    'f1_score':[ f1score_rf , f1score_rf_Best],\"ROC_AUC\":[ AUC_RF , AUC_RF_Best]}\n",
        "result=pd.DataFrame(data=data,index=ind)\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBepMsZuPmRX"
      },
      "source": [
        "* As you can see after using **Hyperparameters** *Accuracy , Precision , f1_score , ROC_AUC Increased*(tiny change) , *Recall decresed*<br>\n",
        "* But the change is very low , If u wish then you can ignore it also."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC4vHvHfPmRY"
      },
      "source": [
        "## **Conclusion**\n",
        "### The ML model for the problem statement was created using python with the help of the dataset, and the ML model created with RandomForest model  performed better than Logistics Regression & XGBClassifier . <br>Thus, for the given problem, the models created by Random Forest is preferred.\n",
        "\n",
        "\n",
        "#### 1. Customers of age between 30 to 60 are more likely to buy insurance.\n",
        "\n",
        "#### 2. Customers with Driving License have higher chance of buying Insurance.\n",
        "\n",
        "#### 3. Customers with Vehicle_Damage are likely to buy insurance.\n",
        "\n",
        "#### 4. The variable such as Previously_insured , Vehcile_Damage are more affecting the target variable.\n",
        "\n",
        "#### 5 The variable such as Driving_License ,   Gender  are not affecting the target variable.\n",
        "\n",
        "#### 6. comparing ROC curve we can see that Random Forest model perform better. Because curves closer to the top-left corner, it indicate a better performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJegbm_2a5Xg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}